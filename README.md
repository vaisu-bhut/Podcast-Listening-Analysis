# Podcast Listening Analysis â€“ **Headphoneâ€¯Heroes** ğŸ§

> **Dataset & Evaluation**: Kaggle *Playground Seriesâ€¯S5â€¯E4* â€“ predict minutes listened, scored by **RMSE**. (<https://www.kaggle.com/competitions/playground-series-s5e4>)

A single Jupyter notebook is fine for explorationâ€”but brittle for handâ€‘offs, hyperâ€‘parameter sweeps, and repeatable nightâ€‘runs. This repo shows how to turn the very same Kaggle workflow into a **productionâ€‘grade, containerised data pipeline** without renting a single cloud VM.

---

## âš™ï¸ Tech stack & _why it beats the alternatives_

| Layer | Chosen tech | Why not something else? |
|-------|-------------|-------------------------|
| **Language** | **PythonÂ 3.11** | Ubiquitous in DS; rich ML ecosystem; still fast enough once the heavy lifting is in C++ libs. |
| **Distributed compute / ETL** | **ApacheÂ SparkÂ 3.5 + PySpark** | Handles 10â¶â€“10â· rows on a laptop and 10â¹ on a cluster with the same code. Pandas or Polars choke at scale or require explicit sharding. |
| **Model** | **XGBoostâ€‘SparkÂ 1.7** | Stateâ€‘ofâ€‘theâ€‘art for tabular data; supports both CPU & GPU; integrates with Sparkâ€™s DataFrame API â†’ zero glue code. LightGBM needs a separate cluster or ugly Pandas fallâ€‘back. |
| **Orchestration** | **ApacheÂ AirflowÂ 2.9** | Mature DAG scheduler, rich UI, sensors, retries. Cron scripts break on failure; Prefect/Covalent fresh but less battleâ€‘tested. |
| **Containerisation** | **Docker + dockerâ€‘compose** | Guarantees â€œruns on my machineâ€ parity, even on CI; compose spins up all services with a single command. Kubernetes is overkill for two containers. |
| **Storage** | **Parquet + Arrow IPC** | Columnar, compressed, seeks only needed columns; Arrow keeps zeroâ€‘copy interoperability with pandas for quick plots. CSV is 6Ã— slower & fatter. |
| **Hyperâ€‘parameter tuning** | **Optuna** (optional) | Asynchronous TPE sampler works outâ€‘ofâ€‘theâ€‘box with Sparkâ€™s driver program; no extra service required. |
| **Lint / CI** | **preâ€‘commit + Ruff + Black** | Keeps PRs clean and buildâ€‘readyâ€”fast (<0.1â€¯s) toolchain. |

---

## ğŸ—‚ï¸ Repo layout (TL;DR)
```
Podcast-Listening-Analysis/
â”œâ”€â”€ airflow/          # Custom Airflow image & DAGs
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ pipeline_dag.py   # Schedules the Spark job
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ spark-app/        # All heavy lifting lives here
â”‚   â”œâ”€â”€ pipeline.py   # ETL â†’ FE â†’ train â†’ predict â†’ export
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ data/             # Drop Kaggle CSVs here; outputs land here too
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

---

## ğŸ”„ How the pipeline flows
1. **Airflow DAG** fires daily (or onâ€‘push).
2. **Spark job** inside `spark-app` container reads `/data/train.csv` & `/data/test.csv`.
3. **Feature engineering**
   * `is_weekend`, `pub_hour`, `host_guest_ratio`, `ads_per_minute`, `sentiment_score`â€¦
4. **Model training**: Sparkâ€‘XGBoost â€“ depthâ€¯8, lrâ€¯0.05, 2â€¯000 rounds, earlyâ€‘stopping on 20â€¯% validation split.
5. **Prediction** on test â†’ Parquet â†’ CSV.
6. **(Optional)** last task uploads via Kaggle CLI if an API token is present.

Everything is **stateless**. Kill the containers â†’ spin again â†’ identical artefacts.

---

## ğŸ§± Extending the baseline
* **Text embeddings** â€“ drop `Episode_Description` into `spark-nlp` â†’ Universalâ€¯Sentenceâ€¯Encoder.
* **Ensembles** â€“ schedule multiple models (LightGBM, CatBoost) and blend.
* **Autoâ€‘tune** â€“ wrap Optuna study around hyperâ€‘params; Airflow can parallelise trials across task instances.
* **Monitoring** â€“ add MLflow tracking server container; log metrics & artefacts automatically.

PRs welcome â€“ follow **ConventionalÂ Commits** and run `preâ€‘commit run -a` before pushing.

---

## ğŸ“œ License
This project is licensed under the [MIT License](LICENSE). Feel free to use, modify, and distribute it as you see fit.

> Data Â©â€¯Kaggle (synthetic). Code Â©Â the contributors.